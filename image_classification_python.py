# -*- coding: utf-8 -*-
"""ACML PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mg-cPCVQBu_34LvjOUviWg2TjimL7pt8

# Setup
"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
import tensorflow_datasets as tfds
from IPython.display import display

# for time
import time as t

"""# Load dataset"""

CIFAR10, CIFAR10_info = tfds.load('cifar10', split=['train', 'test'], as_supervised=True, with_info=True)

"""Dataset info"""

display(CIFAR10_info)

print(CIFAR10_info.features["label"].num_classes)
print(CIFAR10_info.features["label"].names)

"""Set number of classes for later"""

k = CIFAR10_info.features["label"].num_classes

"""Get image dimensions"""

features = CIFAR10_info.features
display(features)

h, w, f = features['image'].shape
h, w, f

"""### Unpack into training and testing"""

for dataset in CIFAR10:
  print(type(dataset))
  print(len(dataset))
  print()

D_train, D_test = CIFAR10

"""Create validation dataset


"""

D_val = D_train.take(int(0.1*len(D_train)))
D_train = D_train.skip(int(0.1*len(D_train)))

print(len(D_val))
print(len(D_train))

"""### View some samples

Get a sense of what labels look like
"""

it = D_train.as_numpy_iterator()
sample = it.next()

print(len(sample)) # should be 2, the image and label
print(sample[0].shape) # the image
print(sample[1]) # the label

"""Visualise data"""

tfds.as_dataframe(D_train.take(10), CIFAR10_info)

"""### Build a training pipeline

The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; and we should opt to make our input values smaller. We will thus standardize values to be in the [0, 1] range.
"""

def normalize_img(image, label):
  """Normalizes images: `uint8` -> `float32`."""
  return tf.cast(image, tf.float32) / 255., label

D_train = D_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
D_train = D_train.cache()
D_train = D_train.shuffle(CIFAR10_info.splits['train'].num_examples)
D_train = D_train.batch(128)
D_train = D_train.prefetch(tf.data.AUTOTUNE)

"""### Build an evaluation pipeline"""

D_val = D_val.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
D_val = D_val.batch(128)
D_val = D_val.cache()
D_val = D_val.prefetch(tf.data.AUTOTUNE)

"""### Build a testing pipeline"""

D_test = D_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
D_test = D_test.batch(128)
D_test = D_test.cache()
D_test = D_test.prefetch(tf.data.AUTOTUNE)

"""# Create and train models

### Loss and Optimizer functions

We will choose the tf.keras.optimizers.Adam optimizer and tf.keras.losses.SparseCategoricalCrossentropy loss function. 

Adam is a popular optimizer for neural networks and is a good choice for deep neural networks & large datasets, because it can converge quickly under the given conditions.

Sparse Categorical Crossentropy is a commonly used loss function for multiclass classification problems. We will also use it because our target variable is represented as integers (instead of one-hot encoded vectors).

To view training and validation accuracy for each training epoch, we can simply pass the metrics argument to Model.compile.
"""

ADAM = tf.keras.optimizers.Adam()
SCCE = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) 
SCA = [tf.keras.metrics.SparseCategoricalAccuracy()]

"""The Lion Optimizer"""

# Copyright 2023 Google Research. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""TF2 implementation of the Lion optimizer."""

import tensorflow.compat.v2 as tf


class Lion(tf.keras.optimizers.legacy.Optimizer):
  r"""Optimizer that implements the Lion algorithm."""

  def __init__(self,
               learning_rate=0.0001,
               beta_1=0.9,
               beta_2=0.99,
               wd=0,
               name='lion',
               **kwargs):
    """Construct a new Lion optimizer."""

    super(Lion, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self._set_hyper('wd', wd)

  def _create_slots(self, var_list):
    # Create slots for the first and second moments.
    # Separate for-loops to respect the ordering of slot variables from v1.
    for var in var_list:
      self.add_slot(var, 'm')

  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(Lion, self)._prepare_local(var_device, var_dtype, apply_state)

    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    wd_t = tf.identity(self._get_hyper('wd', var_dtype))
    lr = apply_state[(var_device, var_dtype)]['lr_t']
    apply_state[(var_device, var_dtype)].update(
        dict(
            lr=lr,
            beta_1_t=beta_1_t,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            one_minus_beta_2_t=1 - beta_2_t,
            wd_t=wd_t))

  @tf.function(jit_compile=True)
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or
                    self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    var_t = var.assign_sub(
        coefficients['lr_t'] *
        (tf.math.sign(m * coefficients['beta_1_t'] +
                      grad * coefficients['one_minus_beta_1_t']) +
         var * coefficients['wd_t']))
    with tf.control_dependencies([var_t]):
      m.assign(m * coefficients['beta_2_t'] +
               grad * coefficients['one_minus_beta_2_t'])

  @tf.function(jit_compile=True)
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or
                    self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    m_t = m.assign(m * coefficients['beta_1_t'])
    m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']
    m_t = m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))
    var_t = var.assign_sub(coefficients['lr'] *
                           (tf.math.sign(m_t) + var * coefficients['wd_t']))

    with tf.control_dependencies([var_t]):
      m_t = m_t.scatter_add(tf.IndexedSlices(-m_scaled_g_values, indices))
      m_t = m_t.assign(m_t * coefficients['beta_2_t'] /
                       coefficients['beta_1_t'])
      m_scaled_g_values = grad * coefficients['one_minus_beta_2_t']
      m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))

  def get_config(self):
    config = super(Lion, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'wd': self._serialize_hyperparameter('wd'),
    })
    return config

LION = Lion()

"""Define function for visualising model performance"""

def visualise(model_history, accuracy_metric, epochs):
  acc = model_history.history[accuracy_metric]
  val_accuracy_metric = 'val_'+accuracy_metric
  val_acc = model_history.history[val_accuracy_metric]

  loss = model_history.history['loss']
  val_loss = model_history.history['val_loss']

  epochs_range = range(epochs)

  plt.figure(figsize=(8, 5))
  plt.subplot(1, 2, 1)
  plt.plot(epochs_range, acc, label='Training Accuracy')
  plt.plot(epochs_range, val_acc, label='Validation Accuracy')
  plt.legend(loc='lower right')
  plt.title('Training and Validation Accuracy')

  plt.subplot(1, 2, 2)
  plt.plot(epochs_range, loss, label='Training Loss')
  plt.plot(epochs_range, val_loss, label='Validation Loss')
  plt.legend(loc='upper right')
  plt.title('Training and Validation Loss')
  plt.show()

"""## Traditional Neural Network

Creating the model
"""

neural_net = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(h, w, f)),
  tf.keras.layers.Dense(1024, activation='sigmoid'),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dense(256, activation='sigmoid'),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(k)
])

neural_net.compile(
    optimizer = tf.keras.optimizers.Adam(),
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]
)

neural_net.summary()

"""Train model, epochs=25, optimizer=ADAM"""

epochs = 25

t1 = t.time()
neural_net_history = neural_net.fit(
    D_train,
    validation_data=D_val,
    epochs=epochs,
)
t2 = t.time()
print(t2-t1)

loss, accuracy = neural_net.evaluate(D_test)

it = D_test.as_numpy_iterator()

sample = it.next()

print(type(sample))
print(len(sample))
print(len(sample[0]))
print(len(sample[1]))

x = sample[0][0]
y_true = sample[1][0]

y_pred = neural_net.predict(tf.expand_dims(input=x, axis=0))
y_pred

"""Visualise model performance"""

visualise(neural_net_history, 'sparse_categorical_accuracy', epochs)

"""Our model consists of 5 layers; 4 hidden layers and 1 output layer with no activation function. The first and third layer are sigmoid activated and the other are two relu activated. It also uses the Adam optimizer, SCCE loss function and SCA as an accuracy metric.

The use of sigmoid and relu activation functions, as opposed to the simpler linear activation function, allows our Neural Network model (of 3,837,066 parameters) to reach an accuracy of at least 48%. This is quite low, as this means that our classification model most likely fails 52% of the time. But is very expected and understandable of a Traditional Neural Network.

At over 10 epochs, out of a total of 25 and taking approximately 55 seconds to finish, we also start noticing some overfitting over the training data. The model isn't as good at classifying the validation data, as it is with the training data.

### Traditional Neural Network 2
"""

neural_net_2 = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(h, w, 3)),
  tf.keras.layers.Dense(2048, activation='sigmoid'),
  tf.keras.layers.Dense(4096, activation='relu'),
  tf.keras.layers.Dense(2048, activation='sigmoid'),
  tf.keras.layers.Dense(1024, activation='sigmoid'),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dense(256, activation='sigmoid'),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(k)
])

neural_net_2.compile(
    optimizer = tf.keras.optimizers.Adam(),
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]
    )

neural_net_2.summary()

epochs = 20

t1 = t.time()
neural_net_2_history = neural_net_2.fit(
    D_train,
    validation_data=D_val,
    epochs=epochs,
)
t2 = t.time()
print(t2-t1)

loss, accuracy = neural_net_2.evaluate(D_test)

visualise(neural_net_2_history, 'sparse_categorical_accuracy', epochs)

"""Our next model consists of 3 more hidden layers for a total of 8. This time, only the 2nd, 5th and 7th layers are relu activated, the rest are sigmoid. Even though it also uses the Adam optimizer, SCCE and SCA; because we use more sigmoid functions, and from having more layers and param eters in general, we expect our training to take longer. 

The more epochs and parameters we use, the higher the accuracy with respect to the training data. But also, with the more epochs we use, the more our traditional neural network model tends overfits to the training data. Not much overfitting is shown by our model, but at an accuracy of 30%, a loss of 1.88 and over 25 000 000 parameters; the model is able to poorly classify the training data, as relatively good as the validation data. Meaning it is most likely to make the wrong prediction approximately 70% of the time.

### Traditional neural Network 3
"""

neural_net_3 = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(h, w, 3)),
  tf.keras.layers.Dense(2048, activation='sigmoid'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(4096, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(2048, activation='sigmoid'),
  tf.keras.layers.Dense(1024, activation='sigmoid'),
  tf.keras.layers.Dense(512, activation='relu'),
  tf.keras.layers.Dense(256, activation='sigmoid'),
  tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(k)
])

neural_net_3.compile(
    optimizer = LION,
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]
    )

neural_net_3.summary()

epochs = 25
t1 = t.time()
neural_net_3_history = neural_net_3.fit(
    D_train,
    validation_data=D_val,
    epochs=epochs,
)
t2 = t.time()
print(t2-t1)

loss, accuracy = neural_net_3.evaluate(D_test)

visualise(neural_net_3_history, 'sparse_categorical_accuracy', epochs)

"""For our third and final TNN model, we keep the layers exactly the same as our previous model, except now we add some regularization. We add dropout regularization to the first, second and last hidden layers; and then add ridge regression to the last hidden layer. We also use a different, faster converging optimizer; the Lion optimizer; instead of Adam.

At 25 epochs, with an accuracy and loss of about 45% and 1.5 respectively, the model shows very little under and/or overfitting to the training data. This due to the high number of parameters (25,865,354), the Lion optimizer, the use of sigmoid and relu activation functions, and the added regularization which is normally used to particularly reduce overfitting. 

We notice this is more accurate than the previous model, but given it still just below even 50\% accuracy; shows the TNNs inherintly inefficient image classification capabilitites.

## Convolutional Neural Network

### Create baseline model CNN1 with less convolutional and pooling layers.

Our baseline model is a simple architecture that has only a few convolutional layers, filters and pooling layers. This approach may be suitable for simpler image classification tasks or datasets with less variation in features. We will begin with no regularization just as before with the TNNs. We will also begin with compiling with the ADAM optimizer.
"""

CNN1 = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(24, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(48, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(),

      tf.keras.layers.Flatten(),
        
      tf.keras.layers.Dense(1024, activation='relu'),
        
      tf.keras.layers.Dense(k),
    ],
    name = 'CNN1'
)

CNN1.compile(
  optimizer = tf.keras.optimizers.Adam(),
  loss = SCCE,
  metrics = SCA
)

"""Model summary"""

CNN1.summary()

"""Train the model"""

epochs = 25

CNN1_history = CNN1.fit(
  D_train,
  validation_data = D_val,
  epochs = epochs
)

"""Evaluate model"""

loss, accuracy = CNN1.evaluate(D_test)

"""Visualize training results"""

visualise(CNN1_history, 'sparse_categorical_accuracy', epochs)

"""We see that already, the CNN already achieves about 67.30% test accuracy with about 2x less parameters as the traditional NN. We notice maximum accuracy scores of about 71% during training with validation loss < 0.9 in some epochs before overfitting kicks in. These values were not achieved by our most tweaked TNN.

The plots show that training and validation accuracy are off by large margins, as well as the training and validation losses. This is due to the overfitting that kicks in at about 5 epochs, which also causes model accuracy to drop

Hence, we will thus try to increase the overall performance of the model as we did with the TNNs.

#### CNN1 with dropout

To reduce overfitting and the large margins of discrapencies between training and validation loss and accuracy, we also introduce dropout regularization to the network. We do this in hopes of retaining some of the accuracy for the duration of the training.

We thus create another CNN1 but with dropout regularization
"""

CNN1_dropout = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(24, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(48, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(),

      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(1024, activation='relu'),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(k),
    ],
    name = 'CNN1_dropout'
)

CNN1_dropout.compile(
  optimizer = tf.keras.optimizers.Adam(),
  loss = SCCE,
  metrics = SCA
)

"""We train the model with the same 25 epochs"""

epochs = 25

CNN1_droput_history = CNN1_dropout.fit(
  D_train,
  validation_data = D_val,
  epochs = epochs
)

loss, accuracy = CNN1_dropout.evaluate(D_test)

"""Visualise training results"""

visualise(CNN1_droput_history, 'sparse_categorical_accuracy', epochs)

"""We have managed to retain most of the validation accuracy, attaining a test accuracy of about 73.87%. However, this architecture still peaks at this accuracy level and continues to overfit. This leads us to add more complexity to the model.

#### CNN1 with L2 regularization
"""

l2_reg = tf.keras.regularizers.l2(0.00025)

CNN1_L2 = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(24, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(48, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D(),

      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(1024, kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(k),
    ],
    name = 'CNN1_L2'
)

CNN1_L2.compile(
  optimizer = tf.keras.optimizers.Adam(),
  loss = SCCE,
  metrics = SCA
)

epochs = 50

CNN1_L2_hist = CNN1_L2.fit(
  D_train,
  validation_data = D_val,
  epochs = epochs
)

loss, accuracy = CNN1_L2.evaluate(D_test)

visualise(CNN1_L2_hist, 'sparse_categorical_accuracy', epochs)

"""We set our regularization factor (a hyperparameter) to 0.00025 and applied L2 regularization to just 2 layers for now. This made training take a bit longer, that is, training accuracy did not quickly climb to > 90% as we saw previously and needed about 2x as much epochs to train. This is because the model is trying to minimize both the loss and the regularization term. Consequently, the divergence in the performance measurenent metrics was also reduced by a noticable margin. This helped the model achieve a test accuracy of about 75.14%.

### CNN2 : Added Convolutional & Pooling Layers

We have already seen the capabilities of CNNs in image classification from CNN1. Hence, in our next model, we aim for higher accuracy by ramping up the convolutional and pooling layers, as well as the filters . This enables the network to capture more complex patterns and features from the input images. This can be beneficial when dealing with more intricate image classification tasks or datasets with high variation in features.
"""

l2_reg = tf.keras.regularizers.l2(0.0005)

CNN2 = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(48, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(128, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(),
        
      tf.keras.layers.Conv2D(256, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D(strides=(2,2)),
        
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(2048, kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(k), 
    ],
    name = 'CNN2'
)

"""We compile the model with the Adam otimizer."""

CNN2.compile(
  optimizer = tf.keras.optimizers.Adam(),
  loss = SCCE,
  metrics = SCA
)

"""Model summary"""

CNN2.summary()

"""We train the model with the same 25 epochs"""

epochs = 50

CNN2_his = CNN2.fit(
  D_train,
  validation_data=D_val,
  epochs=epochs
)

loss, accuracy = CNN2.evaluate(D_test)

"""Visualise training results"""

visualise(CNN2_his, 'sparse_categorical_accuracy', epochs)

"""This time we set our regularization factor to 0.0005 and added l2 regularization in alternating layers in the network since it now has more layers. As expected, adding more complexity to the model improved its validation accuracy, we see it reaching 80% during training and a final test accuracy of 80.85% as well.

### CNN2 with Lion optimizer

We now try the Lion optimizer on the same network, since it has demonstrated that it can aid tasks such as image classification attain better results. We investigate that with our next model.
"""

l2_reg = tf.keras.regularizers.l2(0.0005)

CNN2_L = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(48, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(128, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(),
        
      tf.keras.layers.Conv2D(256, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D(strides=(2,2)),
        
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(2048, kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(k)
    ],
    name = 'CNN2_L'
)

CNN2_L.compile(
  optimizer = LION,
  loss = SCCE,
  metrics = SCA
)

"""We now stop training at 20 epochs since we have observed that our models already diverge in terms of their performance measurement metrics by then."""

epochs = 50

CNN2L_his = CNN2_L.fit(
  D_train,
  validation_data=D_val,
  epochs=epochs
)

loss, accuracy = CNN2_L.evaluate(D_test)

visualise(CNN2L_his, 'sparse_categorical_accuracy', epochs)

"""As expected, the Lion optimizer has aided in increasing the accuracy of our model. We also notice how "smoother" the curves have become, indicating that training was more stable with the Lion optimizer. The final test accuracy has increased to 82.97%

### Model 3 : Increased Filter Sizes

Increasing the filter size in convolutional layers allows the network to capture larger and more global features in the input images. This can be beneficial when dealing with larger images or when the target objects or patterns have a broader spatial extent.
"""

l2_reg = tf.keras.regularizers.l2(0.001)

CNN3 = tf.keras.Sequential(
    layers = [
      tf.keras.layers.Input(shape=(h, w, 3)),
        
      tf.keras.layers.Conv2D(96, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D((4,4), strides=(1,1)),
        
      tf.keras.layers.Conv2D(256, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(),
        
      tf.keras.layers.Conv2D(384, 4, padding='same', kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.MaxPooling2D(strides=(1,1)),
        
      tf.keras.layers.Conv2D(256, 4, padding='same', activation='relu'),
      tf.keras.layers.MaxPooling2D(strides=(2,2)),
        
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(4096, kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.Dropout(0.4),
        
      tf.keras.layers.Dense(4096, kernel_regularizer=l2_reg, activation='relu'),
      tf.keras.layers.Dropout(0.3),
        
      tf.keras.layers.Dense(k), 
    ],
    name = 'CNN3'
)

CNN3.compile(
  optimizer = LION,
  loss = SCCE,
  metrics = SCA
)

CNN3.summary()

epochs = 75

CNN3_hist = CNN3.fit(
  D_train,
  validation_data=D_val,
  epochs=epochs
)

loss, accuracy = CNN3.evaluate(D_test)

visualise(CNN3_hist, 'sparse_categorical_accuracy', epochs)